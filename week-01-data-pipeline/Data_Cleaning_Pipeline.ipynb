{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMs6/RPg/kGahEldnSDazza",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/happyahluwalia/llm_journey/blob/main/week-01-data-pipeline/Data_Cleaning_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üìö LLM Data Cleaning Pipeline - Week 1\n",
        "# Full tutorial: https://github.com/happyahluwalia/llm_journey\n",
        "\n",
        "# Install dependencies (only needed in Colab)\n",
        "!pip install trafilatura langdetect -q\n",
        "\n",
        "print(\"‚úÖ Setup complete! Ready to start.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_d-0s6QbGiH",
        "outputId": "83c59fd5-4e29-45ee-82fc-b94af1a7443e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Setup complete! Ready to start.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìö What This Notebook Covers\n",
        "\n",
        "**Concepts explained:** All steps of a production data cleaning pipeline\n",
        "\n",
        "**Code implemented:** Core foundational steps (collection through quality filtering)\n",
        "**Advanced topics:** Deduplication and content filtering are explained conceptually;\n",
        "implementations will be covered in advanced tutorials\n",
        "\n",
        "*This approach lets you understand the full pipeline without getting overwhelmed\n",
        "by complex algorithms like MinHash or toxicity classifiers.*\n",
        "\n",
        "One of the critical steps before training or using any data is preprocessing. Data in its raw format is seldom usable. It needs to undergo cleaning before we can ingest it into our system.\n",
        "\n",
        "## Core Pipeline\n",
        "\n",
        "1. **Data Collection** ‚Äî Gather from sources\n",
        "2. **Extraction** ‚Äî Get plain text (method varies by source)\n",
        "3. **Language Detection** ‚Äî Filter to target language(s)\n",
        "4. **Quality Filtering (Heuristics)** ‚Äî Remove obvious junk\n",
        "   - Length filters\n",
        "   - Gibberish detection\n",
        "   - High repetition\n",
        "   - Spam patterns\n",
        "5. **Exact Deduplication** ‚Äî Remove identical documents\n",
        "6. **Fuzzy Deduplication** ‚Äî Remove near-duplicates (modern pipelines)\n",
        "\n",
        "## Optional Steps (Depends on Your Goals)\n",
        "\n",
        "7. **URL/Domain Filtering** ‚Äî Block known bad domains (RefinedWeb style)\n",
        "8. **Toxicity Filtering** ‚Äî Remove harmful content (if needed)\n",
        "9. **Quality Scoring** ‚Äî Score each document (Dolma style)\n",
        "10. **PII Removal** ‚Äî Remove personal info (usually left to users)\n",
        "\n",
        "Remember **\"There's no standard pipeline - it depends on goals\"**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### üß© Real-World Pipelines Using These Steps\n",
        "\n",
        "The datasets we're sampling from ‚Äî Wikipedia, StackExchange, GitHub, Common Crawl ‚Äî are used in many large-scale open-source corpora:\n",
        "\n",
        "- **Dolma (Allen AI)** ‚Äî Metadata-enriched, multilingual corpus built from Common Crawl, Wikipedia, and structured sources\n",
        "- **RefinedWeb & FineWeb (Hugging Face)** ‚Äî Filtered Common Crawl with quality scoring and aggressive deduplication\n",
        "- **The Pile (EleutherAI)** ‚Äî Foundational dataset used for GPT-NeoX, includes Books3, ArXiv, PubMed, StackExchange\n",
        "\n",
        "**Our goal:** Simulate a smaller-scale version of these pipelines, understanding each step deeply.\n",
        "\n",
        "---\n",
        "\n",
        "## üìã What We'll Cover\n",
        "\n",
        "Below, we'll walk through each step with:\n",
        "- ‚úÖ **Why it matters** ‚Äî The problem it solves\n",
        "- ‚úÖ **How it works** ‚Äî The techniques used\n",
        "- ‚úÖ **Trade-offs** ‚Äî Precision vs. recall, speed vs. quality\n",
        "- ‚úÖ **Real examples** ‚Äî How production pipelines implement it\n",
        "\n",
        "Let's start with **Data Collection**..."
      ],
      "metadata": {
        "id": "RsNGBTrYGzqs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Data Collection**\n",
        "\n",
        "One of the first steps when you start any data project is to decide **what data sources** to use.\n",
        "\n",
        "This naturally raises two key questions:\n",
        "\n",
        "> **Question 1:** \"What do I want my model or chatbot to *do*?\"\n",
        "\n",
        "- Should it be **general-purpose**, able to talk about anything?  \n",
        "- Or should it be **domain-specialized**, e.g., a Legal assistant, Medical expert, or Coding Ninja?\n",
        "\n",
        "> **Question 2:** \"What kind of *attitude* or *voice* do I want my model to have?\"\n",
        "\n",
        "- Chatty and conversational, like Reddit?  \n",
        "- Or factual and formal, like Wikipedia?\n",
        "\n",
        "---\n",
        "\n",
        "## üß≠ Before You Jump In ‚Äî Ask These Questions\n",
        "\n",
        "1. **What is my end goal?** (Chat, Q&A, summarization, code generation)\n",
        "2. **How wide should my data coverage be?** (Broad vs. deep)\n",
        "3. **How much domain knowledge vs. general reasoning is required?**\n",
        "4. **What's my tolerance for noise, bias, and licensing complexity?**\n",
        "5. **How much data do I need?** (1GB for fine-tuning vs. 1TB for pre-training)\n",
        "\n",
        "---\n",
        "\n",
        "## ‚öôÔ∏è When to Use What\n",
        "\n",
        "| Model Type | Data Sources | Description |\n",
        "|-------------|---------------|--------------|\n",
        "| **General Purpose** | Web + Books + Dialog + Code | Broad, diverse mix (Wikipedia, Reddit, GitHub, Books3) |\n",
        "| **Domain Specialized**<br>(Finance, Legal, Medical) | Curated licensed datasets | Higher precision and quality, less noise<br>(e.g., PubMed, Legal archives, Financial reports) |\n",
        "| **Code Model** | GitHub, StackOverflow, CodeSearchNet | Code and explanations; focus on structure and correctness |\n",
        "\n",
        "---\n",
        "\n",
        "## üåê Common Data Sources (With Trade-offs)\n",
        "\n",
        "| Category | Examples | Notes |\n",
        "|-----------|-----------|-------|\n",
        "| **Web Crawls** | ‚Ä¢ **Common Crawl** ‚Äì 400+ TB of raw web data<br>‚Ä¢ **RefinedWeb** ‚Äì Filtered Common Crawl (Hugging Face)<br>‚Ä¢ **FineWeb** ‚Äì Even cleaner version (Hugging Face)<br>‚Ä¢ **Dolma** ‚Äì Metadata-rich (Allen AI) | **Trade-off:** Scale vs. quality<br>Raw Common Crawl is 95% noise; use pre-filtered versions |\n",
        "| **Code** | ‚Ä¢ GitHub<br>‚Ä¢ StackOverflow<br>‚Ä¢ CodeSearchNet<br>‚Ä¢ The Stack (Hugging Face) | **Best for:** Models that reason about code, logic, debugging<br>**Watch out for:** Licensing (some repos are proprietary) |\n",
        "| **Community Q&A** | ‚Ä¢ Reddit<br>‚Ä¢ Quora<br>‚Ä¢ StackExchange | **Adds:** Conversational tone, diverse phrasing, slang<br>**Watch out for:** Toxicity, misinformation |\n",
        "| **Curated Knowledge** | ‚Ä¢ Wikipedia<br>‚Ä¢ ArXiv (academic papers)<br>‚Ä¢ Books3<br>‚Ä¢ Project Gutenberg | **Best for:** Factual grounding, high-quality prose<br>**Trade-off:** Low noise but limited diversity |\n",
        "\n",
        "---\n",
        "\n",
        "## üîÑ How to Get the Data\n",
        "\n",
        "### **Option 1: Use Pre-Built Datasets**\n",
        "\n",
        "‚úÖ **Pros:**\n",
        "- Instant access via Hugging Face, ready to use\n",
        "- Pre-filtered for quality\n",
        "- Well-documented\n",
        "\n",
        "‚ùå **Cons:**\n",
        "- Limited customization\n",
        "- May be outdated\n",
        "- Can't control the collection process\n",
        "\n",
        "### **Option 2: Web APIs (For structured, real-time data)**\n",
        "‚úÖ Pros:\n",
        "\n",
        "- Clean, structured data (JSON)\n",
        "- Officially supported, legal clarity\n",
        "- Rate limits are transparent\n",
        "\n",
        "‚ùå Cons:\n",
        "\n",
        "- Requires API keys\n",
        "- Rate limits (e.g., Reddit: 60 req/min)\n",
        "- May have usage costs\n",
        "\n",
        "Best for: Reddit, GitHub, Twitter/X, StackExchange\n",
        "\n",
        "### **Option 3: Web Scraping (For custom sources without APIs)**\n",
        "‚úÖ Pros:\n",
        "\n",
        "- Full control over what you collect\n",
        "- Free (no API costs)\n",
        "- Can access any public website\n",
        "\n",
        "‚ùå Cons:\n",
        "\n",
        "- Fragile (breaks when HTML changes)\n",
        "- Legal gray area (check robots.txt, ToS)\n",
        "- Risk of IP bans if too aggressive\n",
        "- Requires more technical work\n",
        "\n",
        "Best for: Niche forums, blogs, specialized websites\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "##**‚öñÔ∏è Legal & Ethical Considerations**\n",
        "\n",
        "‚ö†Ô∏è Always check the license of each dataset.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "vhoM5jTYXRpE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Cell: Fetch HTML content from multiple sources for data cleaning demo ---\n",
        "\n",
        "import requests\n",
        "from pathlib import Path\n",
        "import time\n",
        "\n",
        "# Config: Base directory and size limit (4 MB per source)\n",
        "BASE_DIR = Path(\"data/raw\")\n",
        "source_sizes = {\"wiki\": 0, \"github\": 0, \"stackexchange\": 0}\n",
        "\n",
        "# Create directories for each source\n",
        "for sub in [\"wiki\", \"github\", \"stackexchange\"]:\n",
        "    (BASE_DIR / sub).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Headers to avoid rate-limiting\n",
        "headers = {\"User-Agent\": \"Mozilla/5.0 (compatible; DataPipelineBot/1.0)\"}\n",
        "\n",
        "# 1Ô∏è‚É£ Wikisource: Fetch 4 HTML public domain pages (stories/essays)\n",
        "print(\"Fetching 4 Wikisource HTML pages...\")\n",
        "wiki_urls = [\n",
        "    (\"The Raven by Edgar Allan Poe\", \"https://en.wikisource.org/wiki/The_Raven\"),\n",
        "    (\"Rip Van Winkle by Washington Irving\", \"https://en.wikisource.org/wiki/Rip_Van_Winkle\"),\n",
        "    (\"The Fall of the House of Usher by Edgar Allan Poe\", \"https://en.wikisource.org/wiki/The_Fall_of_the_House_of_Usher\"),\n",
        "    (\"A Modest Proposal by Jonathan Swift\", \"https://en.wikisource.org/wiki/A_Modest_Proposal\")\n",
        "]\n",
        "try:\n",
        "    sample_text = \"\"  # Store last text for sample\n",
        "    for i, (title, url) in enumerate(wiki_urls):\n",
        "        resp = requests.get(url, headers=headers)\n",
        "        if resp.status_code != 200:\n",
        "            print(f\"Error fetching '{title}': {resp.status_code}\")\n",
        "            continue\n",
        "        text_bytes = html_text.encode(\"utf-8\")\n",
        "        filename = f\"wiki_{i+1}_{title.replace(' ', '_').replace(' by ', '_')[:50]}.html\"\n",
        "        (BASE_DIR / \"wiki\" / filename).write_bytes(text_bytes)\n",
        "        source_sizes[\"wiki\"] += len(text_bytes)\n",
        "        sample_text = html_text  # Update sample\n",
        "        print(f\"  - Downloaded: {title} ({len(text_bytes)/1024:.1f} KB)\")\n",
        "        time.sleep(1)  # Polite delay\n",
        "except Exception as e:\n",
        "    print(f\"Wikisource fetch failed: {e}\")\n",
        "\n",
        "# 2Ô∏è‚É£ GitHub: Fetch 3-4 HTML-rendered READMEs or docs from transformers repo\n",
        "print(\"Fetching 3-4 GitHub HTML pages...\")\n",
        "GITHUB_BASE = \"https://github.com/huggingface/transformers/raw/main\"  # Raw file base URL\n",
        "GITHUB_FILES = [\n",
        "    (\"README\", \"/README.md\"),  # Main README\n",
        "    (\"BERT Docs\", \"/docs/source/en/model_doc/bert.md\"),  # Model doc\n",
        "    (\"GPT-2 Docs\", \"/docs/source/en/model_doc/gpt2.md\"),\n",
        "    (\"T5 Docs\", \"/docs/source/en/model_doc/t5.md\")\n",
        "]\n",
        "try:\n",
        "    sample_text = \"\"  # Reset for GitHub\n",
        "    files_downloaded = 0  # Track to ensure 3-4 files\n",
        "    for i, (name, path) in enumerate(GITHUB_FILES):\n",
        "        url = GITHUB_BASE + path\n",
        "        resp = requests.get(url, headers=headers)\n",
        "        if resp.status_code != 200:\n",
        "            print(f\"Error fetching '{name}': {resp.status_code}\")\n",
        "            continue\n",
        "        # Convert Markdown to HTML-like content (for cleaning demo)\n",
        "        html_text = f\"<html><body><h1>{name}</h1><pre>{resp.text[:50_000]}</pre></body></html>\"\n",
        "        text_bytes = html_text.encode(\"utf-8\")\n",
        "        filename = f\"github_{i+1}_{name.replace(' ', '_')}.html\"\n",
        "        (BASE_DIR / \"github\" / filename).write_bytes(text_bytes)\n",
        "        source_sizes[\"github\"] += len(text_bytes)\n",
        "        files_downloaded += 1\n",
        "        sample_text = html_text\n",
        "        print(f\"  - Downloaded: {name} ({len(text_bytes)/1024:.1f} KB)\")\n",
        "        time.sleep(1)\n",
        "except Exception as e:\n",
        "    print(f\"GitHub fetch failed: {e}\")\n",
        "\n",
        "# 3Ô∏è‚É£ StackExchange: Fetch 5 Python-tagged Q&A posts\n",
        "print(\"Fetching 5 StackExchange Q&A posts...\")\n",
        "so_url = \"https://api.stackexchange.com/2.3/questions?order=desc&sort=activity&tagged=python&site=stackoverflow&filter=withbody\"\n",
        "try:\n",
        "    resp = requests.get(so_url, headers=headers)\n",
        "    if resp.status_code == 200:\n",
        "        posts = resp.json().get(\"items\", [])[:5]  # Limit to 5\n",
        "        for i, post in enumerate(posts):\n",
        "            content = f\"Title: {post.get('title', '')}\\n\\nBody: {post.get('body', '')}\"\n",
        "            content_bytes = content.encode(\"utf-8\")\n",
        "            (BASE_DIR / \"stackexchange\" / f\"post_{i+1}.html\").write_bytes(content_bytes)\n",
        "            source_sizes[\"stackexchange\"] += len(content_bytes)\n",
        "            print(f\"  - Downloaded: Post {i+1} ({len(content_bytes)/1024:.1f} KB)\")\n",
        "            time.sleep(1)\n",
        "    else:\n",
        "        print(f\"StackExchange API error: {resp.status_code}\")\n",
        "except Exception as e:\n",
        "    print(f\"StackExchange fetch failed: {e}\")\n",
        "\n",
        "# Summary\n",
        "print(f\"‚úÖ Done. Files in 'data/raw/*'\")\n",
        "print(f\"Total sizes: Wiki={source_sizes['wiki']/1_000_000:.2f} MB, \"\n",
        "      f\"GitHub={source_sizes['github']/1_000_000:.2f} MB, \"\n",
        "      f\"StackExchange={source_sizes['stackexchange']/1_000_000:.2f} MB\")"
      ],
      "metadata": {
        "id": "rKLG4offdm9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##**üöÄ What's Next?**\n",
        "Once you've collected your data, it's rarely in a usable format. Raw data contains:\n",
        "\n",
        "- HTML tags, scripts, CSS\n",
        "- Duplicate content\n",
        "- Low-quality or spam content\n",
        "- Multiple languages mixed together\n",
        "- Encoding issues\n",
        "\n",
        "**Next steps in the pipeline:**\n",
        "\n",
        "- **Extraction** ‚Äî Get plain text from raw formats\n",
        "- **Cleaning** ‚Äî Remove noise, duplicates, low-quality content\n",
        "- **Filtering** ‚Äî Keep only relevant, high-quality data\n",
        "\n",
        "Let's dive into **Extraction** next..."
      ],
      "metadata": {
        "id": "7B1GudwrWjE3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Data Extraction**\n",
        "\n",
        "Once we have the data available, it is rarely in a usable format. We need to extract the actual data and ignore things like HTML tags, scripts, non-text elements, etc., to be able to use only the relevant text.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## üîß The Extraction Pipeline\n",
        "\n",
        "Most modern systems use a **multi-step approach**:\n",
        "\n",
        "1. **Rule-based extraction** ‚Üí Remove all HTML tags, scripts, styles\n",
        "2. **Heuristic techniques** ‚Üí Score content blocks using:\n",
        "   - Text density (ratio of text to HTML)\n",
        "   - Link density (ratio of links to text)\n",
        "   - DOM structure analysis\n",
        "3. **Optional ML-based extraction** ‚Üí Refine results for edge cases\n",
        "\n",
        "### Popular Tool: **[Trafilatura](https://github.com/adbar/trafilatura)**\n",
        "\n",
        "A production-ready Python library that combines rules + heuristics for fast, accurate extraction.\n",
        "\n",
        "**What it does:**\n",
        "- Removes HTML boilerplate (headers, footers, ads, navigation)\n",
        "- Extracts main article/post content\n",
        "- Preserves paragraph structure\n",
        "- Supports 50+ languages\n",
        "- Fast enough for large-scale processing\n",
        "\n",
        "**Used by:** Dolma, various research projects, production web scrapers\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Key Concepts to Understand\n",
        "\n",
        "### **1. Precision vs. Recall Tradeoff**\n",
        "\n",
        "| Approach | Strategy | Pros | Cons | Example |\n",
        "|----------|----------|------|------|---------|\n",
        "| **High Precision**<br>(Conservative) | Extract only what you're confident about | ‚úÖ Low noise<br>‚úÖ High quality | ‚ùå Miss good content | Extract only from `<article>` tags |\n",
        "| **High Recall**<br>(Aggressive) | Extract anything that *might* be content | ‚úÖ Capture more content<br>‚úÖ Better coverage | ‚ùå More noise | Extract all `<p>` tags |\n",
        "\n",
        "**Modern Approach:**  \n",
        "‚ú® **Go with High Recall** ‚Üí Extract more, filter later  \n",
        "\n",
        "üíæ Storage is cheap, missing good data is expensive\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "### **2. Extraction vs. Cleaning**\n",
        "\n",
        "These are **different stages** in the pipeline:\n",
        "\n",
        "| Stage | Focus | What It Does | Example |\n",
        "|-------|-------|--------------|---------|\n",
        "| **Extraction** | Structure | Get text from raw data format | Remove `<script>` tags from HTML |\n",
        "| **Cleaning** | Content | Improve text quality | Remove duplicates, fix encoding, filter profanity |\n",
        "\n",
        "    \n",
        "**Order matters:**\n",
        "\n",
        "*Raw Data -> Extraction -> Cleaning -> Training Ready Text*\n",
        "\n",
        "\n",
        "- **Extraction happens first** ‚Üí It's about the *structure* of the data\n",
        "- **Cleaning happens next** ‚Üí It's about the *content* of the data\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## üí° Key Insight\n",
        "\n",
        "> **The best extraction technique depends on the source of the data.**\n",
        "\n",
        "Different sources need different strategies:\n",
        "- Wikipedia dumps ‚Üí Already clean, minimal extraction needed\n",
        "- Reddit/Forums ‚Üí Extract posts/comments via API (structured)\n",
        "- General web ‚Üí Aggressive HTML extraction + heavy filtering\n",
        "- Code repositories ‚Üí Syntax-aware extraction\n",
        "\n",
        "---\n",
        "###**‚ö†Ô∏è Why Extraction Quality Matters**\n",
        "Poor extraction leads to:\n",
        "\n",
        " - ‚ùå Training on navigation menus (\"Home | About | Contact\" appears thousands of times)\n",
        " - ‚ùå Learning ad language (\"Buy now! Limited time offer!\")\n",
        " - ‚ùå Memorizing boilerplate (\"Copyright 2024. All rights reserved.\")\n",
        " - ‚ùå Wasted compute on junk tokens\n",
        "\n",
        "Good extraction means:\n",
        "\n",
        "- ‚úÖ Model learns from actual content\n",
        "- ‚úÖ Better reasoning and generation quality\n",
        "- ‚úÖ Less memorization of meaningless patterns\n",
        "- ‚úÖ More efficient training (fewer junk tokens)\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OWl4magwooME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Text Extraction\n",
        "%pip install trafilatura\n",
        "import trafilatura\n",
        "from pathlib import Path\n",
        "\n",
        "# Config: Input/output directories and size limit (4 MB per source)\n",
        "BASE_DIR = Path(\"data/raw\")\n",
        "OUT_DIR = Path(\"data/extracted\")\n",
        "MAX_SIZE_BYTES = 4_000_000  # 4 MB in bytes\n",
        "source_sizes = {\"wiki\": 0, \"github\": 0, \"stackexchange\": 0}\n",
        "\n",
        "# Create output directories for each source\n",
        "for sub in [\"wiki\", \"github\", \"stackexchange\"]:\n",
        "    (OUT_DIR / sub).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Process each source directory\n",
        "for source in [\"wiki\", \"github\", \"stackexchange\"]:\n",
        "    print(f\"Extracting content from {source} HTML files...\")\n",
        "    input_dir = BASE_DIR / source\n",
        "    output_dir = OUT_DIR / source\n",
        "\n",
        "    # Get all .html files in source directory\n",
        "    html_files = list(input_dir.glob(\"*.html\"))\n",
        "    if not html_files:\n",
        "        print(f\"No HTML files found in {input_dir}\")\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        for html_file in html_files:\n",
        "            # Check size limit before processing\n",
        "            if source_sizes[source] > MAX_SIZE_BYTES * 0.9:\n",
        "                print(f\"Stopping {source} extraction: Approaching 4 MB limit\")\n",
        "                break\n",
        "\n",
        "            # Read HTML content\n",
        "            html_content = html_file.read_text(encoding=\"utf-8\")\n",
        "\n",
        "            # Extract main content with Trafilatura\n",
        "            extracted_text = trafilatura.extract(\n",
        "                html_content,\n",
        "                include_formatting=True,  # Plain text, no markup\n",
        "                include_links=False,       # Exclude hyperlinks\n",
        "                include_tables=False,      # Exclude table content\n",
        "                deduplicate=False           # Remove duplicate content\n",
        "            ) or \"\"  # Fallback to empty string if extraction fails\n",
        "\n",
        "            # Limit to ~50 KB chars (UTF-8 ~50 KB bytes)\n",
        "            extracted_text = extracted_text[:50_000]\n",
        "            text_bytes = extracted_text.encode(\"utf-8\")\n",
        "\n",
        "            if source_sizes[source] + len(text_bytes) > MAX_SIZE_BYTES:\n",
        "                print(f\"Skipping {html_file.name}: Exceeds 4 MB limit for {source}\")\n",
        "                continue\n",
        "\n",
        "            # Save extracted text\n",
        "            output_file = output_dir / f\"{html_file.stem}.txt\"\n",
        "            output_file.write_bytes(text_bytes)\n",
        "            source_sizes[source] += len(text_bytes)\n",
        "\n",
        "            print(f\"Extracted: {html_file.name} ({len(text_bytes)/1024:.1f} KB)\")\n",
        "\n",
        "        # Print sample from last extracted file (if any)\n",
        "        if extracted_text:\n",
        "            print(f\"Sample extracted text from {source} (first 500 chars): {extracted_text[:500]}...\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {source}: {e}\")\n",
        "\n",
        "# Summary\n",
        "print(f\"‚úÖ Done. Extracted files in 'data/extracted/*'\")\n",
        "print(f\"Total sizes: Wiki={source_sizes['wiki']/1_000_000:.2f} MB, \"\n",
        "      f\"GitHub={source_sizes['github']/1_000_000:.2f} MB, \"\n",
        "      f\"StackExchange={source_sizes['stackexchange']/1_000_000:.2f} MB\")"
      ],
      "metadata": {
        "id": "izA2F-YQ-Ha3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üöÄ What's Next?\n",
        "\n",
        "After extraction, we have plain text ‚Äî but it's still messy!\n",
        "\n",
        "**Next step**: **Data Cleaning**\n",
        "- Remove duplicates\n",
        "- Filter low-quality content\n",
        "- Handle encoding issues\n",
        "- Language detection\n",
        "- Quality scoring"
      ],
      "metadata": {
        "id": "-mB9rvSzWpUN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Data Cleaning**\n",
        "\n",
        "We now have data extracted from all the sources. However, we cannot use it currently as it needs to be cleaned.\n",
        "\n",
        "## ‚ùì Why Clean Data?\n",
        "\n",
        "Raw extracted text still contains major issues:\n",
        "\n",
        "| Issue | Problem | Impact |\n",
        "|-------|---------|--------|\n",
        "| **Duplicates** | Same content repeated multiple times | Leads to memorization and overfitting |\n",
        "| **Near-duplicates** | Similar but not exact copies | Model learns same patterns repeatedly |\n",
        "| **Multiple languages** | Content not in target language | Wasted training on irrelevant data |\n",
        "| **Low quality content** | Spam, gibberish, auto-generated text | Degrades model performance |\n",
        "| **Encoding issues** | `CafÔøΩ`, `√¢‚Ç¨‚Ñ¢`, mojibake | Creates noise in vocabulary |\n",
        "| **Harmful content** | Toxicity, profanity | Safety concerns |\n",
        "| **PII data** | Personal information | Privacy and legal issues |\n",
        "\n",
        "---\n",
        "\n",
        "## üîÑ The Cleaning Pipeline\n",
        "\n",
        "Modern systems use this multi-step approach:\n",
        "```\n",
        "Extracted Text\n",
        "     ‚Üì\n",
        "Text Normalization\n",
        "     ‚Üì\n",
        "Language Detection & Filtering\n",
        "     ‚Üì\n",
        "Quality Filtering\n",
        "     ‚Üì\n",
        "URL-level Deduplication\n",
        "     ‚Üì\n",
        "Exact Deduplication\n",
        "     ‚Üì\n",
        "Fuzzy Deduplication\n",
        "     ‚Üì\n",
        "(Optional) Content Filtering\n",
        "     ‚Üì\n",
        "Clean Training Data\n",
        "\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üõ†Ô∏è Step 1: Text Normalization\n",
        "\n",
        "**What it does:** Fix basic text issues before processing.\n",
        "\n",
        "**Common fixes:**\n",
        "- Remove excessive whitespace and normalize line breaks\n",
        "- Fix encoding errors (convert all to UTF-8)\n",
        "- Remove special/invisible characters\n",
        "- Standardize punctuation and formatting\n",
        "\n",
        "**Why it matters:** Makes downstream steps (especially deduplication) more accurate.\n",
        "\n",
        "---\n",
        "\n",
        "## üåç Step 2: Language Detection & Filtering\n",
        "\n",
        "**What it does:** Identify and keep only content in your target language(s).\n",
        "\n",
        "**How:** Use libraries like **fastText language detector**\n",
        "- Trained on 176 languages from Wikipedia\n",
        "- Fast and accurate\n",
        "- Returns language + confidence score\n",
        "\n",
        "**Decision:** Set confidence threshold (e.g., keep only documents with >70% confidence in target language)\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÇÔ∏è Step 3: Quality Filtering\n",
        "\n",
        "**What it does:** Remove low-quality content using rule-based filters.\n",
        "\n",
        "### Filter by Size:\n",
        "- **Too short:** < 200 characters (likely fragments)\n",
        "- **Too long:** > 100K words (likely corrupted or auto-generated)\n",
        "\n",
        "### Filter by Repetition:\n",
        "- **Line-level:** Remove if same line repeats 3+ times\n",
        "- **Word-level:** Remove excessive duplication (\"the the the\")\n",
        "\n",
        "### Filter by Content Patterns:\n",
        "- Copyright notices: \"¬© 2024 All rights reserved\"\n",
        "- Navigation: \"Home | About | Contact\"\n",
        "- Boilerplate: \"This site uses cookies\"\n",
        "- Placeholder: \"Lorem ipsum\", \"Coming soon\"\n",
        "\n",
        "---\n",
        "\n",
        "## üîÑ Step 4: Deduplication (Exact)\n",
        "\n",
        "**What it does:** Remove duplicate URLs and identical documents.\n",
        "\n",
        "### URL-level:\n",
        "- Hash each URL\n",
        "- Store in a set\n",
        "- Skip if already seen\n",
        "\n",
        "### Content-level:\n",
        "- Hash the full document\n",
        "- Use set or Bloom filter to track\n",
        "- Remove if hash exists\n",
        "\n",
        "**Impact:** Removes 15-30% of data typically.\n",
        "\n",
        "---\n",
        "\n",
        "### üí° Key Insight:\n",
        "\n",
        "> **Exact deduplication is necessary but not sufficient.**\n",
        "\n",
        "Many documents are *similar* but not *identical*:\n",
        "- Same article with minor edits\n",
        "- Different formatting of same content\n",
        "- Paraphrased versions\n",
        "\n",
        "This is where **fuzzy deduplication** becomes critical.\n",
        "\n",
        "---\n",
        "\n",
        "## üîç Step 5: Fuzzy Deduplication (Near-Duplicates)\n",
        "\n",
        "**What it does:** Remove documents that are similar but not exactly the same.\n",
        "\n",
        "### Why This Is Critical:\n",
        "\n",
        "Near-duplicates cause:\n",
        "- ‚ùå Memorization of repeated patterns\n",
        "- ‚ùå Wasted compute training on same content\n",
        "- ‚ùå Overfitting to specific phrasings\n",
        "- ‚ùå Reduced effective dataset diversity\n",
        "\n",
        "**Impact:** Modern pipelines remove 50-70% of remaining data!\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ Major Insight (2023-2024):\n",
        "\n",
        "**Old thinking:**\n",
        "- More data = better models\n",
        "\n",
        "**New understanding:**\n",
        "- More **unique** data = better models\n",
        "- Training on 500GB deduplicated > 1TB raw data\n",
        "- Aggressive fuzzy dedup often **improves** model quality\n",
        "\n",
        "**Key learning:** Quality > Quantity\n",
        "\n",
        "---\n",
        "\n",
        "## üõ°Ô∏è Step 6: Content Filtering (Optional)\n",
        "\n",
        "**What it does:** Remove harmful, toxic, or sensitive content.\n",
        "\n",
        "### Common Filters:\n",
        "\n",
        "#### **Toxicity & Profanity:**\n",
        "- Use word lists or ML classifiers\n",
        "- Trade-off: Safety vs. over-filtering\n",
        "\n",
        "#### **PII (Personal Information):**\n",
        "- Remove emails, phone numbers, addresses\n",
        "- Use regex patterns or NER models\n",
        "- Challenge: Slow at scale, many false positives\n",
        "\n",
        "#### **Adult Content:**\n",
        "- Domain blocklists\n",
        "- Content classifiers\n",
        "\n",
        "**Note:** Many datasets skip this and leave filtering to end users.\n",
        "\n",
        "---\n",
        "\n",
        "## üìä Real-World Impact\n",
        "\n",
        "### Example: Common Crawl ‚Üí FineWeb\n",
        "\n",
        "```\n",
        "10 TB    Common Crawl (raw)\n",
        "  ‚Üì Extraction\n",
        "7 TB     (30% removed: HTML, scripts)\n",
        "  ‚Üì Language Detection\n",
        "5 TB     (30% removed: non-English)\n",
        "  ‚Üì Quality Filtering\n",
        "3.5 TB   (30% removed: low quality)\n",
        "  ‚Üì Exact Deduplication\n",
        "2.5 TB   (25% removed: duplicates)\n",
        "  ‚Üì Fuzzy Deduplication\n",
        "1 TB     (60% removed: near-duplicates!)\n",
        "\n",
        "```\n",
        "\n",
        "**Result:** 90% removed, but the remaining 10% is far higher quality!\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Key Takeaways\n",
        "\n",
        "1. **Cleaning is not optional** ‚Äî Raw data will hurt model performance\n",
        "2. **Deduplication is the most impactful step** ‚Äî Removes 50-70% of data but improves quality\n",
        "3. **Fast heuristics > slow ML** ‚Äî Rule-based filters work well and scale\n",
        "4. **No one-size-fits-all** ‚Äî Different use cases need different cleaning strategies\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fTfcYdZ1CZvr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 1: Text Normalization**"
      ],
      "metadata": {
        "id": "qQIPIC4zStcz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Text Normalization\n",
        "import re\n",
        "import unicodedata\n",
        "from pathlib import Path\n",
        "\n",
        "# Config: Input/output directories and size limit\n",
        "IN_DIR = Path(\"data/extracted\")\n",
        "OUT_DIR = Path(\"data/cleaned\")\n",
        "source_sizes = {\"wiki\": 0, \"github\": 0, \"stackexchange\": 0}\n",
        "\n",
        "# Create output directories\n",
        "for sub in [\"wiki\", \"github\", \"stackexchange\"]:\n",
        "    (OUT_DIR / sub).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Normalize text: whitespace, encoding, punctuation, special chars\n",
        "def normalize_text(text):\n",
        "    # Convert to UTF-8, normalize Unicode (e.g., fix 'Caf√©' vs 'CafÔøΩ')\n",
        "    text = unicodedata.normalize(\"NFKC\", text)\n",
        "    # Remove excessive whitespace and normalize line breaks\n",
        "    text = re.sub(r'\\s+', ' ', text.strip())\n",
        "    # Standardize punctuation (e.g., curly quotes to straight)\n",
        "    text = re.sub(r'[‚Äò‚Äô]', \"'\", text)\n",
        "    text = re.sub(r'[‚Äú‚Äù]', '\"', text)\n",
        "    # Remove special/invisible characters (e.g., control chars)\n",
        "    text = re.sub(r'[^\\x20-\\x7E\\n]', '', text)\n",
        "    return text\n",
        "\n",
        "# Process each source\n",
        "for source in [\"wiki\", \"github\", \"stackexchange\"]:\n",
        "    print(f\"Normalizing {source} text files...\")\n",
        "    input_dir = IN_DIR / source\n",
        "    output_dir = OUT_DIR / source\n",
        "\n",
        "    # Get all .txt files\n",
        "    txt_files = list(input_dir.glob(\"*.txt\"))\n",
        "\n",
        "    try:\n",
        "        sample_text = \"\"  # For sample output\n",
        "        for txt_file in txt_files:\n",
        "            # Read and normalize text\n",
        "            text = txt_file.read_text(encoding=\"utf-8\")\n",
        "            normalized_text = normalize_text(text)\n",
        "            text_bytes = normalized_text.encode(\"utf-8\")\n",
        "\n",
        "             # Save normalized text\n",
        "            output_file = output_dir / f\"{txt_file.stem}_normalized.txt\"\n",
        "            output_file.write_bytes(text_bytes)\n",
        "            source_sizes[source] += len(text_bytes)\n",
        "            sample_text = normalized_text\n",
        "\n",
        "            print(f\"   - Normalized: {txt_file.name} ({len(text_bytes)/1024:.1f} KB)\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error normalizing {source}: {e}\")\n",
        "\n",
        "print(f\"‚úÖ Normalized files in 'data/cleaned/*'\")\n",
        "print(f\"Total sizes: Wiki={source_sizes['wiki']/1_000_000:.2f} MB, \"\n",
        "      f\"GitHub={source_sizes['github']/1_000_000:.2f} MB, \"\n",
        "      f\"StackExchange={source_sizes['stackexchange']/1_000_000:.2f} MB\")\n"
      ],
      "metadata": {
        "id": "afcRZk4MCcc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 2 - Language detect**"
      ],
      "metadata": {
        "id": "88DmZlRPSpDL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2 - Language detect\n",
        "# using langdetect in place of fastText as its easier to demo in notebook\n",
        "\n",
        "%pip install langdetect\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from langdetect import detect_langs\n",
        "\n",
        "# Config: Input/output directories and size limit\n",
        "IN_DIR = Path(\"data/cleaned\")\n",
        "OUT_DIR = Path(\"data/cleaned\")\n",
        "source_sizes = {\"wiki\": 0, \"github\": 0, \"stackexchange\": 0}\n",
        "\n",
        "# Process each source\n",
        "for source in [\"wiki\", \"github\", \"stackexchange\"]:\n",
        "    print(f\"Filtering {source} for English text...\")\n",
        "    input_dir = IN_DIR / source\n",
        "    output_dir = OUT_DIR / source\n",
        "\n",
        "    txt_files = list(input_dir.glob(\"*_normalized.txt\"))\n",
        "    if not txt_files:\n",
        "        print(f\"No normalized files found in {input_dir}\")\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        sample_text = \"\"\n",
        "        for txt_file in txt_files:\n",
        "\n",
        "            text = txt_file.read_text(encoding=\"utf-8\")\n",
        "            # Detect language with confidence\n",
        "            try:\n",
        "                langs = detect_langs(text)\n",
        "                is_english = any(lang.lang == \"en\" and lang.prob >= 0.7 for lang in langs)\n",
        "                if not is_english:\n",
        "                    print(f\"Skipping {txt_file.name}: Non-English or low confidence\")\n",
        "                    continue\n",
        "            except Exception as e:\n",
        "                print(f\"Skipping {txt_file.name}: Language detection failed ({e})\")\n",
        "                continue\n",
        "\n",
        "            text_bytes = text.encode(\"utf-8\")\n",
        "\n",
        "            # Save English text\n",
        "            output_file = output_dir / f\"{txt_file.stem}_en.txt\"\n",
        "            output_file.write_bytes(text_bytes)\n",
        "            source_sizes[source] += len(text_bytes)\n",
        "            sample_text = text\n",
        "\n",
        "            print(f\"  - Filtered: {txt_file.name} ({len(text_bytes)/1024:.1f} KB, English)\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error filtering {source}: {e}\")\n",
        "\n",
        "print(f\"‚úÖ English-filtered files in 'data/cleaned/*'\")\n",
        "print(f\"Total sizes: Wiki={source_sizes['wiki']/1_000_000:.2f} MB, \"\n",
        "      f\"GitHub={source_sizes['github']/1_000_000:.2f} MB, \"\n",
        "      f\"StackExchange={source_sizes['stackexchange']/1_000_000:.2f} MB\")"
      ],
      "metadata": {
        "id": "3ks9lAnWCnYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Step 3: Quality Filtering**"
      ],
      "metadata": {
        "id": "Ie2rE4hsC8eq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Quality Filtering\n",
        "from pathlib import Path\n",
        "import re\n",
        "\n",
        "# Config: Input/output directories and size limit\n",
        "IN_DIR = Path(\"data/cleaned\")\n",
        "OUT_DIR = Path(\"data/cleaned\")\n",
        "source_sizes = {\"wiki\": 0, \"github\": 0, \"stackexchange\": 0}\n",
        "\n",
        "# Quality filters\n",
        "def is_high_quality(text):\n",
        "    # Too short: < 200 chars\n",
        "    if len(text) < 200:\n",
        "        return False, \"Too short (< 200 chars)\"\n",
        "    # Too long: > 100K words (rough estimate: chars / 5)\n",
        "    if len(text.split()) > 100_000 / 5:\n",
        "        return False, \"Too long (> 100K words)\"\n",
        "    # Repetitive lines: same line 3+ times\n",
        "    lines = text.split(\"\\n\")\n",
        "    if any(lines.count(line) >= 3 for line in set(lines) if line.strip()):\n",
        "        return False, \"Repetitive lines\"\n",
        "    # Repetitive words: \"the the the\" patterns\n",
        "    if re.search(r'\\b(\\w+)\\s+\\1\\s+\\1\\b', text, re.IGNORECASE):\n",
        "        return False, \"Repetitive words\"\n",
        "    # Boilerplate patterns\n",
        "    boilerplate = [\n",
        "        r\"copyright.*all rights reserved\",\n",
        "        r\"home\\s*\\|\\s*about\\s*\\|\\s*contact\",\n",
        "        r\"this site uses cookies\",\n",
        "        r\"lorem ipsum\"\n",
        "    ]\n",
        "    if any(re.search(pattern, text, re.IGNORECASE) for pattern in boilerplate):\n",
        "        return False, \"Boilerplate content\"\n",
        "    return True, \"Passed\"\n",
        "\n",
        "# Process each source\n",
        "for source in [\"wiki\", \"github\", \"stackexchange\"]:\n",
        "    print(f\"Quality filtering {source} files...\")\n",
        "    input_dir = IN_DIR / source\n",
        "    output_dir = OUT_DIR / source\n",
        "\n",
        "    txt_files = list(input_dir.glob(\"*_en.txt\"))\n",
        "    if not txt_files:\n",
        "        print(f\"No English files found in {input_dir}\")\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        sample_text = \"\"\n",
        "        for txt_file in txt_files:\n",
        "\n",
        "            text = txt_file.read_text(encoding=\"utf-8\")\n",
        "            is_quality, reason = is_high_quality(text)\n",
        "            if not is_quality:\n",
        "                print(f\"Skipping {txt_file.name}: {reason}\")\n",
        "                continue\n",
        "\n",
        "            text_bytes = text.encode(\"utf-8\")\n",
        "\n",
        "            output_file = output_dir / f\"{txt_file.stem}_quality.txt\"\n",
        "            output_file.write_bytes(text_bytes)\n",
        "            source_sizes[source] += len(text_bytes)\n",
        "            sample_text = text\n",
        "\n",
        "            print(f\"  - Filtered: {txt_file.name} ({len(text_bytes)/1024:.1f} KB)\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error filtering {source}: {e}\")\n",
        "\n",
        "print(f\"‚úÖ Quality-filtered files in 'data/cleaned/*'\")\n",
        "print(f\"Total sizes: Wiki={source_sizes['wiki']/1_000_000:.2f} MB, \"\n",
        "      f\"GitHub={source_sizes['github']/1_000_000:.2f} MB, \"\n",
        "      f\"StackExchange={source_sizes['stackexchange']/1_000_000:.2f} MB\")"
      ],
      "metadata": {
        "id": "fZc_Gmf1C9uE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üöÄ Next Steps: Going Deeper\n",
        "\n",
        "Now that you understand the data cleaning pipeline, here are ways to expand:\n",
        "\n",
        "### **Implement Deduplication (Intermediate)**\n",
        "- **Exact dedup:** Use Python's `hashlib` to hash documents\n",
        "- **Fuzzy dedup:** Explore libraries like `datasketch` (MinHash) or `text-dedup`\n",
        "- **Project idea:** Build a dedup tool and measure impact on a small dataset\n",
        "\n",
        "### **Add Content Filtering (Intermediate)**\n",
        "- **Toxicity:** Use `detoxify` library or Perspective API\n",
        "- **PII:** Regex patterns for emails/phones, or `presidio-analyzer`\n",
        "- **Project idea:** Create a safety filter for your specific use case\n",
        "\n",
        "### **Scale Up (Advanced)**\n",
        "- Process full Common Crawl snapshots\n",
        "- Implement distributed deduplication with Spark\n",
        "- Build quality scoring models\n"
      ],
      "metadata": {
        "id": "DcyyCauUUzjb"
      }
    }
  ]
}